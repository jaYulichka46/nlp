# Lab 1: Data Collection & Audit

**Задача:** Класифікація текстів (Task A - Text Classification).
**Поле `text`:** Містить повний текст новин українською мовою. Кожному тексту відповідає певна категорія (Target), така як "Політика", "Спорт", "Економіка" тощо.

## Як запустити ноутбук в Colab
1. Відкрийте ноутбук з папки `notebooks/` у Google Colab.
2. Виконайте першу клітинку для встановлення бібліотеки `ua-datasets` та завантаження даних.
3. Запустіть решту клітинок для проведення аудиту та первинної нормалізації.

## правила очистки/нормалізації
У межах першої лабораторної роботи реалізовано базовий рівень очищення:
* **Уніфікація апострофа:** Всі варіації символу апострофа (`'`, `` ` ``, `‘`) замінено на стандартний `’`.
* **Маскування контактів:** Всі URL-посилання та Email-адреси замінено на відповідні теги `<URL>` та `<EMAIL>`.
* **Видалення зайвого:** Прибрано множинні пробіли та символи переносу рядків для отримання суцільного тексту.

## 5 найболючіших edge cases у датасеті
На основі проведеного аудиту виявлено наступні проблеми:
1. **Точні дублікати:** Наявність ідентичних текстів новин, що може викривити результати навчання.
2. **Надто короткі тексти:** Новини, що містять менше 5 слів (наприклад, лише заголовок або технічна помилка), які не несуть семантичного навантаження.
3. **"Сміттєві" рядки:** Рядки, що складаються виключно з цифр, знаків пунктуації або спецсимволів.
4. **HTML-артефакти:** Залишки тегів та нерозривних пробілів після парсингу сайтів новин.
5. **Дисбаланс класів:** Нерівномірний розподіл кількості новин за категоріями (наприклад, політики значно більше, ніж культури).

## Що стало краще “після”
* **Кількісний аудит:** Оброблено повний масив даних обсягом **120 417 новин**.
* **Якість даних:** Виявлено та відфільтровано дублікати, що складають певний відсоток від загальної вибірки.
* **Готовність до аналізу:** Сформовано файл `labels.csv` з усіма унікальними категоріями та створено першу версію очищеного датасету для подальшої роботи.